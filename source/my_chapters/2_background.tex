
\cleardoublepage

\chapter{Background}
\label{background}

address at transplanter?  One-time shot if things mess up.  Difficult to identify plants.  Constant stopping, too much motion and difficult shading make its not ideal for images. 

\section{Similar Research}

%2.4.1.	Beam Splitting 
%2.4.1.1.	Include figure from other paper
%2.4.1.2.	Thereâ€™s a paper about mapping seeds, should I cover that too?
          % should be trying to solve at transplanter instead of post? 
%2.4.2.	Image Based Mapping
%2.4.2.1.	Include figure from other paper

\section{Coordinate Systems}

A key choice in any mapping process is the correct choice of coordinate frames.  In the context of mapping a coordinate system defines the location of an item with respect to another item.  These coordinate systems, or frames of reference, are typically fixed to items there are significant in the mapping process which include the Earth, field, platform and cameras.  The specific coordinate systems used in this research are discussed in the following sections.

\subsection{Latitude and Longitude}

A common frame that is fixed to the Earth is a spherical coordinate frame, with the angles referred to as Latitude and Longitude. However, since the Earth is not a sphere these angles are projected onto an ellipsoid, called a datum.  The datum that is used in this research is the World Geodetic System (WGS-84) which is the default datum used by the Global Positioning System (GPS) <TODO cite?>.  The third dimension is altitude, or elevation, is measured relative to the ellipsoid. 

\subsection{Universal Transverse Mercator (UTM)}

In order to describe the points on the Earth's surface in a two-dimension plane, as is desired for mapping, a projection model must be used on the angular latitude and longitude coordinates.  The projection model used in this research is known as the Universal Transverse Mercator (UTM) which splits the Earth into 60 lateral bands as shown in the figure below.  In each zone an item is described by it's Easting and Northing coordinates.  

\subsection{Field Coordinate System}

One issue with Easting and Northing described by UTM is that rows in a fields are not always planted north and south or east and west.  Many of the post-processing steps benefit from removing this relative field orientation, so a new field coordinate system is defined so the y-axis runs parallel to the rows and increases in the planting direction of the first row, the x-axis increases in the direction of increasing row numbers.  The origin is selected so that all items in the field have positive x and y coordinates.   Similar to UTM the units in this frame are meters.

\subsection{Platform Coordinate System}

Another useful coordinate system is one that's fixed to the platform that's moving through the field.  This platform coordinate system allows the relative spacing and orientation between the cameras and the GNSS <Todo already defined?> antenna to be specified.   Also this coordinate system defines how the platform's orientation is defined in terms of Euler angles, which is useful for accounting for non-level camera orientation. 

The axes of this coordinate system are shown in the figure below <TODO ref>, which defines the x-axis out of the front of the platform, the y-axis out the right hand side and the z-axis orthogonal in the downward direction.  

TODO the discussion for this is outside the scope of this Thesis. 

\subsection{Camera Coordinate System}

The camera coordinate system describes the locations of objects in the world relative to the camera.  Typically a camera coordinate system is defined by the x-axis out of the right hand side of the camera, the y-axis out of the bottom and the z-axis along the optical axis.  <TODO cite>   However for this application an alternative camera coordinate system is defined which has the x-axis out of the top of camera, the y-axis out of the right side and the z-axis along the optical axis.  This is so when the camera is mounted on the platform facing the ground with the top forward, the camera axes will align with the forward-right-down platform coordinate system.  This makes the meaning of the Euler angles consistent.  The origin is defined to be located at the center of the imaging sensor as shown in the image below.

<TODO insert image from dysense wiki>

\section{Coordinate Projections}
 
 An important step in the mapping process is converting between pixel and world coordinates, which is also the subject of many computer vision algorithms.  Since pixels are described in $\mathbb{R}^2$ and world points in $\mathbb{R}^3$, this conversion requires a projection.  
 
 However, before this can be defined a model must be chosen which describes how points in the world are projected onto the imaging sensor.  This section first presents the model used in this research and then discusses two equivalent methods that can be used for converting coordinates.  
 
 \subsection{Projection Model}
 
 A common choice for a thin, convex lens is the central perspective imaging model which can be seen in figure \ref{projection_model}. \
 
<TODO draw coordinate frames.>
 
\begin{figure}[htb]
	\centering
    \includegraphics[height=4.5in]{figures/projection_model.png}
    \caption[Projection model]{Central perspective imaging model showing the image plane in yellow.}
    \label{projection_model}
\end{figure}
 
 This model is based on the concept of an ideal lens that is treated as an equivalent pin-hole, which implies the lens is perfectly focused and has no geometrical distortions.  The ideal lens will have two focal points along the optical axis which are shown in figure \ref{focal_points}.  The central perspective model defines the image plane to be at the focal point in front of the lens, which is the left focal point shown in the figure.  

\begin{figure}[htb]
	\centering
    \includegraphics[height=2.5in]{figures/projections_two_focal.png}
    \caption[Focal points]{Equivalent pin hole lens showing both focal points.}
    \label{focal_points}
\end{figure} 

 Points in the image plane can be described using two different coordinate frames.  The first is the sensor frame and the second is the pixel frame.  
 
 The origin of the sensor frame is located at the point where the optical axis intersects the image plane.  A point in this frame is described by $(x,y)$, where the axes are in the same direction as camera frame and are shown in blue in figure \ref{projection_model}.  
 
 On the other hand, the origin of the pixel frame is located at the top left of the image plane and the x-axis increases to the right and the y-axis increases downwards.  Rather than $(x,y)$ a pixel's location is denoted as $(u,v)$.  The center of the image sensor is referred to as the principal point and is denoted $(u_0,v_0)$.
 
 Another key difference between these frames is the units.  The sensor frame has units of millimeters while the pixel frame has units of pixels.

 \subsection{Projection Methods}

 The conversion between pixel and world coordinates can go both ways, and both are used in the post-processing pipeline.  However, going from pixel to world coordinates is used more often and is slightly more challenging so that is what is presented in this section. The conversion involves three frame transformations   
\begin{center}
 Pixel $(u,v)$ $\rightarrow$ Sensor $(x,y)$ $\rightarrow$ Camera $(X,Y,Z)$ $\rightarrow$ World $(E,N,U)$
\end{center}
 
 This is referred to as a backwards projection, since it is projecting two-dimensional points back out into the world.  As mentioned above this is more challenging because depth information is lost during the forward projection as the image is captured.  There are many ways of recovering this depth information, which typically use multiple images or cameras.  However, the height of objects being mapped are relatively small and is not useful for this mapping application.  For that reason an assumption is made that every point in the world frame has a height of zero.  This reduces the world to a plane and results in a $\mathbb{R}^2$ to $\mathbb{R}^2$ mapping which is possible with a single image. 
 
 There are two different methods for performing this backwards projection. The first method discussed operates in Euclidean space and is more intuitive to understand.  The second method builds on this method by instead working in Projective space which produces a more efficient conversion.
 
 Both of these methods use the central perspective model and both make the same assumptions about the camera and lens.  These are:
 \begin{itemize}
 \item The lens does not cause any distortion. 
 \item There is no skew or error in displacement in how the image sensor is positioned within the camera.
 \item The focal length is exact.
 \end{itemize}

 All of these assumptions are false to some extent, and it's possible to correct these assumptions using certain camera calibration procedures <TODO cite>.  However, for the system presented in this research these effects are considered negligible.
   
 Another assumption is the camera's orientation is relative to the ground.  However, typically orientation angles are measured with respect to the gravity vector, which on hills will not be orthogonal to the ground.  If the mapping is done on a hill then the simplest solution is to make sure the camera's orientation about the platform's X and Y axes is zero.
 
 \subsubsection{Euclidean Space}
 
 The first transformation is to convert pixel coordinates to sensor coordinates.  Using the frame definitions shown in figure \ref{projection_model}, this can be represented in matrix form as 
 
\[
\begin{bmatrix} x \\ y \end{bmatrix}
=
\begin{bmatrix} 
 0 & -\rho_h \\
 \rho_w & 0 
\end{bmatrix}
\begin{bmatrix} u \\ v \end{bmatrix}
+
\begin{bmatrix} \rho_h v_0 \\ \rho_w u_0 \end{bmatrix}
\]
 
 where $\rho_w$ and $\rho_h$ are the width and height of each pixel in millimeters, and $u_0$ and $v_0$ are the  coordinates of the principal point in pixels. 
 
 The next transformation is to convert from sensor coordinates to camera coordinates.  It's clear from the projection model that if the sensor coordinates are combined with the focal length $f$ as
 \begin{center}
 $(x,y,f)$
 \end{center}
 then the result is a vector in the camera frame, but with units of millimeters rather than meters. The last frame transformation is to describe this vector in terms of the world frame.  This can be accomplished by a rotation matrix $R$
 
 \begin{equation}
 \label{rotation_eq}
 \begin{bmatrix} e \\ n \\ u \end{bmatrix}
 =
 R *
 \begin{bmatrix} x \\ y \\ f \end{bmatrix}
 \end{equation}
 
 <TODO> switch to NED instead of UTM. Or just re-arrange.
 
 where this rotation matrix is composed from the active, extrinsic rotations about the worlds X, Y and Z axes in that order, by angles $\phi$ (roll), $\theta$ (pitch) and $\psi$ (yaw).  These are the Tait-Bryan angles of the camera frame with respect to the world frame.  Technically this is a passive rotation by negative angles since the vector is changing frames, but using active rotations avoids a lot of double negative signs.
 
  \begin{align}
  R &= R_z(\psi)*R_y(\theta)*R_x(\phi) \\
    &= \begin{bmatrix} \cos(\psi)  & -\sin(\psi) & 0 \\ 
                        \sin(\psi) & \cos(\psi) & 0 \\
                         0         &   0         & 1  \end{bmatrix}
       \begin{bmatrix} \cos(\theta) & 0 & \sin(\theta) \\ 
                             0      & 1 &       0        \\
                      -\sin(\theta) & 0 & cos(\theta) \end{bmatrix}
       \begin{bmatrix} \ 1 &    0       & 0           \\ 
                         0 & \cos(\phi) & -\sin(\phi) \\
                         0 & \sin(\phi) & cos(\phi)  \end{bmatrix} \\
    &= \begin{bmatrix} \cos(\theta)\cos(\psi) & \sin(\phi)\sin(\theta)\cos(\psi) - \cos(\phi)\sin(\psi) &  \sin(\phi)\sin(\psi) +  \cos(\phi)\sin(\theta)\cos(\psi) \\
    \cos(\theta)\sin(\psi) & \cos(\phi)\cos(\psi) + \sin(\phi)\sin(\theta)\sin(\psi) &  \cos(\phi)\sin(\theta)\sin(\psi) - \sin(\phi)\cos(\psi) \\
     -\sin(\theta) & \sin(\phi)\cos(\theta) & \cos(\phi)\cos(\theta)     \end{bmatrix}
  \end{align}
  
 Lowercase coordinates $(n,e,d)$ in equation \ref{rotation_eq} are used to designate that these components are in the north, east and down directions, but the units are still in millimeters and this does not represent the world point yet.
 
 The position of this vector in the world frame is given by the camera's world position

 \begin{equation}
 T = 
 \begin{bmatrix} T_x \\ T_y \\ T_z \end{bmatrix} =
 \begin{bmatrix} N_{cam} \\ E_{cam} \\ D_{cam} \end{bmatrix}
 \end{equation}

 The actual world point can be found by finding the intersection of this vector (n',e',d') with the flat plane of the Earth given by the equation D=0.  
 
 This can be done by parameterizing the vector with $S$ as 
 
 \begin{equation}
 \label{parameter_ned}
 \begin{bmatrix} N \\ E \\ D \end{bmatrix} =
 S \begin{bmatrix} n' \\ e' \\ d' \end{bmatrix}
 + \begin{bmatrix} T_x \\ T_y \\ T_z \end{bmatrix}
 \end{equation}
 
 plugging in the third component, D, into the plane equation and solving for
 \begin{center}
 $S = -T_z / d'$, 
 \end{center}
 which can be plugged back into equation \ref{parameter_ned}.  If d' is zero then the position vector is parallel to the Earth's surface and will never intersect it.   

 While this method is easy to visualize, it requires multiple steps for each pixel that must be converted to world coordinates.  An alternate method is presented in the next section.

 \subsubsection{Projective Space}
 
 An ideal solution would be to describe the backwards projection from the pixel plane to the world plane as a single matrix that can be applied in one step.  In order to do this, coordinates in each frame must be converted into the Projective space which adds an extra coordinate.  This coordinate represents scale and a set of coordinates in this space are referred to as homogeneous coordinates.

 There are many benefits to using homogeneous coordinates which is why they are commonly used in computer vision.  These benefits include
 \begin{enumerate}
 \item performing translations and rotations in a single matrix.
 \item avoiding unnecessary division in intermediate calculations which is typically slower than other operations.
 \item representing a coordinate at infinity with real numbers which is true when the extra coordinate is zero.
 \end{enumerate}
 
 In order to differentiate between regular Euclidean coordinates, homogeneous coordinates are followed by an apostrophe.  For example coordinates in the pixel frame can be described in Projective space as $(u',v',w')$ where 
 
 \begin{center}
 $u' = u*w'$ and $v'=v*w'$.
 \end{center}
 
 In backwards projection the pixel coordinates $(u,v)$ are what is known so $w'$ is always set to 1.  
 
 In order to convert from pixel to sensor coordinates the same relationship using the pixel sizes and principal point are used, however when homogeneous coordinates are substituted this relationship can be described in a single matrix
 
 \[
 \begin{bmatrix} x' \\ y' \\ z' \end{bmatrix}
 =
 \begin{bmatrix} 
     0   & -\rho_h & \rho_h v_0 \\ 
  \rho_w &    0    & \rho_w u_0 \\
     0   &    0    &      1  
 \end{bmatrix}
 \begin{bmatrix} u' \\ v' \\ 1 \end{bmatrix}
 \]
 
 This is typically referred to as the inverse parameter matrix, or $K^{-1}$.
 
 The second transformation is to convert sensor coordinates to camera coordinates.  A consequence of defining the origin of the camera frame to be the location of the equivalent pin-hole is that all incoming rays of light converge to the origin of the camera frame.  This leads to the simple relationship using the focal length $f$ of
  
  \begin{center}
  $x=fX/Z$  and $y=fY/Z$
  \end{center}
 
  which can easily be derived by similar triangles.  When described as homogeneous coordinates this relationship becomes 
  
  \begin{center}
  $x'=fX$, $y'=fY$ and $z'=Z$
  \end{center}
  
  or in matrix form
  
  [X,Y,Z] = [1/f 1/f 1] [x',y',z']
  
  
  
  
  